{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c7ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d495cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    data_transform = transforms.Compose([transforms.Resize((227,227)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=32) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf0e164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image_matrix):\n",
    "    \"\"\"\n",
    "    Display an image using Matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - image_matrix (numpy.ndarray): NumPy array containing the image data. \n",
    "                                    Should be in the format (channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    image = image_matrix\n",
    "    # Transpose the image array from (channels, height, width) to (height, width, channels) for Matplotlib\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    # Display the image using Matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c506fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_size:float = 0.2):\n",
    "    \"\"\"\n",
    "    Split the training data into training and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_size (float, optional): The proportion of the dataset to include in the validation set (default=0.2).\n",
    "\n",
    "    Returns:\n",
    "    - X_train (numpy.ndarray): NumPy array containing training images.\n",
    "    - X_val (numpy.ndarray): NumPy array containing validation images.\n",
    "    - y_train (numpy.ndarray): NumPy array containing training labels.\n",
    "    - y_val (numpy.ndarray): NumPy array containing validation labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to store validation and training data\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # 1000 because we have 1000 images of each class\n",
    "    samples_per_class_val = (int) (1000 * train_size)\n",
    "    \n",
    "    for class_label in range(10):\n",
    "        # extract indices corresponding to the current class\n",
    "        class_indices = np.where(y_train==class_label)[0]\n",
    "        \n",
    "        # randomly select sample_per_class_val indices for validation\n",
    "        val_indices = np.random.choice(class_indices, samples_per_class_val, replace=False)\n",
    "        \n",
    "        # append the selected val data to X_val and y_val\n",
    "        X_val.extend(X_train[val_indices])\n",
    "        y_val.extend(y_train[val_indices])\n",
    "        \n",
    "        # append the remaining data to X_train and y_train\n",
    "        train_indices = np.setdiff1d(class_indices, val_indices)\n",
    "        X.extend(X_train[train_indices])\n",
    "        y.extend(y_train[train_indices])\n",
    "\n",
    "    # convert python lists to np array\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, X_val, y, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5659394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):\n",
    "    \"\"\"\n",
    "    Shuffle data samples and their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): NumPy array containing data samples.\n",
    "    - y (numpy.ndarray): NumPy array containing corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "    - X_shuffled (numpy.ndarray): NumPy array containing shuffled data samples.\n",
    "    - y_shuffled (numpy.ndarray): NumPy array containing corresponding shuffled labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine X, y into a list of tuples\n",
    "    data = list(zip(X, y))\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Unpack the shuffled data back into separate arrays\n",
    "    X_shuffled, y_shuffled = zip(*data)\n",
    "\n",
    "    # Convert the shuffled lists to NumPy arrays \n",
    "    X_shuffled = np.array(X_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "    \n",
    "    return X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e57fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader from input data and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): Input data array.\n",
    "    - y (numpy.ndarray): Labels array.\n",
    "    - batch_size (int, optional): Batch size for DataLoader (default=32).\n",
    "    - shuffle (bool, optional): Whether to shuffle the data (default=True).\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: PyTorch DataLoader for the input data and labels.\n",
    "    \"\"\"\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # Create a TensorDataset from X_train_tensor and y_train_tensor\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Define batch size and create DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c99e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data():\n",
    "    # Define data augmentation transformations\n",
    "    augmented_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ])\n",
    "\n",
    "    # Apply data augmentation to the original dataset\n",
    "    augmented_dataset = ConcatDataset([train_loader.dataset, train_loader.dataset])\n",
    "\n",
    "    # Create a DataLoader for the combined dataset\n",
    "    combined_loader = DataLoader(augmented_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "\n",
    "    return combined_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f0269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e84f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def train_step(model:torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "\n",
    "    train_loss, train_acc = 0,0\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # put data on target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # calc loss (per batch) and accuracy\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # back pass\n",
    "        loss.backward()\n",
    "\n",
    "        # updating the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5bb0c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_test, y_test in data_loader:\n",
    "            # send data to the target device\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            test_pred = model(X_test)\n",
    "\n",
    "            # calc loss\n",
    "            test_loss += loss_fn(test_pred, y_test)\n",
    "\n",
    "            # calc add\n",
    "            test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
    "\n",
    "        # clac the test loss avg per batch\n",
    "        test_loss /= len(data_loader)\n",
    "\n",
    "        # calc the test acc avg per batch\n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e256b393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    fc_input = 1\n",
    "    def __init__(self, in_channels, out_channels, num_filters, kernel_size, activation_fn, dense_activation_fn, apply_batchnorm, apply_dropout, prob, hidden_units):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define the convolution layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "                               in_channels=in_channels, \n",
    "                               out_channels=num_filters[0], \n",
    "                               kernel_size=kernel_size[0], \n",
    "                               stride=1, \n",
    "                               padding=0\n",
    "                              )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(num_filters[0])\n",
    "        self.conv2 = nn.Conv2d(\n",
    "                               in_channels=num_filters[0],\n",
    "                               out_channels=num_filters[1], \n",
    "                               kernel_size=kernel_size[1], \n",
    "                               stride=1, \n",
    "                               padding=0\n",
    "                              )\n",
    "        self.batchnorm2 = nn.BatchNorm2d(num_filters[1])\n",
    "        self.conv3 = nn.Conv2d(\n",
    "                               in_channels=num_filters[1], \n",
    "                               out_channels=num_filters[2], \n",
    "                               kernel_size=kernel_size[2], \n",
    "                               stride=1, \n",
    "                               padding=0\n",
    "                              )\n",
    "        self.batchnorm3 = nn.BatchNorm2d(num_filters[2])\n",
    "        self.conv4 = nn.Conv2d(\n",
    "                               in_channels=num_filters[2], \n",
    "                               out_channels=num_filters[3], \n",
    "                               kernel_size=kernel_size[3], \n",
    "                               stride=1, \n",
    "                               padding=0\n",
    "                              )\n",
    "        self.batchnorm4 = nn.BatchNorm2d(num_filters[3])\n",
    "        self.conv5 = nn.Conv2d(\n",
    "                               in_channels=num_filters[3],\n",
    "                               out_channels=num_filters[4], \n",
    "                               kernel_size=kernel_size[4], \n",
    "                               stride=1, \n",
    "                               padding=0\n",
    "                              )\n",
    "        self.batchnorm5 = nn.BatchNorm2d(num_filters[4])\n",
    "\n",
    "        # Define activation function\n",
    "        if activation_fn == \"ReLU\": self.activation = nn.ReLU()\n",
    "        elif activation_fn == \"GELU\": self.activation = nn.GELU()\n",
    "        elif activation_fn == \"SiLU\": self.activation = nn.SiLU()\n",
    "        elif activation_fn == \"Mish\": self.activation = nn.Mish()\n",
    "            \n",
    "        # this is for activation func of dense layer\n",
    "        if dense_activation_fn == \"ReLU\": self.dense_activation = nn.ReLU()\n",
    "        elif dense_activation_fn == \"GELU\": self.dense_activation = nn.GELU()\n",
    "        elif dense_activation_fn == \"SiLU\": self.dense_activation = nn.SiLU()\n",
    "        elif dense_activation_fn == \"Mish\": self.dense_activation = nn.Mish()\n",
    "        \n",
    "        # Define max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        input_size = 227\n",
    "        for i in range(5):\n",
    "            input_size = input_size - kernel_size[i] + 1\n",
    "            input_size = input_size // 2\n",
    "        \n",
    "        # Define dense layer\n",
    "        self.fc1 = nn.Linear(input_size*input_size*num_filters[4], hidden_units)\n",
    "        \n",
    "        # Adding Dropout\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        # Define output layer\n",
    "        self.fc2 = nn.Linear(hidden_units, 10)  # 10 output neurons for 10 classes\n",
    "        \n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        self.apply_dropout = apply_dropout\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation, and max pooling layers\n",
    "        x = self.pool(self.activation(self.batchnorm1(self.conv1(x)) if self.apply_batchnorm ==\"Yes\" else self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.batchnorm2(self.conv2(x)) if self.apply_batchnorm ==\"Yes\" else self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.batchnorm3(self.conv3(x)) if self.apply_batchnorm ==\"Yes\" else self.conv3(x)))\n",
    "        x = self.pool(self.activation(self.batchnorm4(self.conv4(x)) if self.apply_batchnorm ==\"Yes\" else self.conv4(x)))\n",
    "        x = self.pool(self.activation(self.batchnorm5(self.conv5(x)) if self.apply_batchnorm ==\"Yes\" else self.conv5(x)))\n",
    "        \n",
    "        # Flatten the output for the dense layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply dense layer and output layer\n",
    "        x = self.dense_activation(self.fc1(x))\n",
    "        \n",
    "        if self.apply_dropout==\"Yes\": x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)  # Apply softmax activation to the output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27526f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5f06ceb7e94214b412b6dcf73c491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da376e4cbec4da6a80816d1f0da5e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/train\"\n",
    "# test_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/val\"\n",
    "\n",
    "# X_train, y_train = read_images(train_path)\n",
    "# X_test, y_test = read_images(test_path)\n",
    "\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_val_split(0.2) \n",
    "\n",
    "# X_train, y_train = shuffle_data(X_train, y_train)\n",
    "# X_val, y_val = shuffle_data(X_val, y_val)\n",
    "\n",
    "# train_loader = create_dataloader(X_train, y_train, 32)\n",
    "# val_loader = create_dataloader(X_val, y_val, 32)\n",
    "# test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63e24ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "\"name\": \"CNN\",\n",
    "\"metric\": {\n",
    "    \"name\":\"val_accuracy\",\n",
    "    \"goal\": \"maximize\"\n",
    "},\n",
    "\"method\": \"bayes\",\n",
    "\"parameters\": {\n",
    "        \"num_filters\": {\n",
    "            \"values\": [32, 64]\n",
    "        },\n",
    "        \"activation_func\": {\n",
    "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
    "        },\n",
    "        \"dense_activation_fn\": {\n",
    "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
    "        },\n",
    "        \"filter_org\": {\n",
    "            \"values\": [\"same\", \"half\", \"double\"]\n",
    "        },\n",
    "        \"data_augment\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"batch_normalization\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"prob\": {\n",
    "            \"values\": [0.2, 0.3]\n",
    "        },\n",
    "        \"filter_size\": {\n",
    "            \"values\": [[3,3,3,3,3]]\n",
    "        },\n",
    "        \"hidden_units\": {\n",
    "            \"values\": [128, 256]\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510c4656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(train_loader, val_loader, config):\n",
    "    epochs = 5\n",
    "    in_channels = 3\n",
    "    out_channels = 10\n",
    "    num_filters = [config.num_filters]\n",
    "    kernel_size = config.filter_size\n",
    "    activation_fn = config.activation_func\n",
    "    dense_activation_fn = config.dense_activation_fn\n",
    "    augment = config.data_augment\n",
    "    filter_org = config.filter_org\n",
    "    batch_norm = config.batch_normalization\n",
    "    dropout = config.dropout\n",
    "    prob = config.prob\n",
    "    hidden_units = config.hidden_units\n",
    "\n",
    "    for i in range(4):\n",
    "        last_value = num_filters[-1]\n",
    "        if(filter_org == \"same\"): num_filters.append(last_value)\n",
    "        elif(filter_org == \"half\"): num_filters.append((int)(last_value * 0.5))\n",
    "        else: num_filters.append(last_value * 2)  \n",
    "\n",
    "    if augment == \"Yes\":\n",
    "        train_loader = augment_data()\n",
    "        \n",
    "    run_name = f\"num_filters_{num_filters[0]}_act_{activation_fn}_filt_org_{filter_org}_augment_{augment}_batchnorm_{batch_norm}_dropout_{dropout}_prob_{prob}_hu_{hidden_units}\"\n",
    "    \n",
    "    model = CNN(\n",
    "                in_channels=in_channels, \n",
    "                out_channels=out_channels, \n",
    "                num_filters=num_filters, \n",
    "                kernel_size=kernel_size, \n",
    "                activation_fn=activation_fn,\n",
    "                dense_activation_fn=dense_activation_fn,\n",
    "                apply_batchnorm=batch_norm,\n",
    "                apply_dropout=dropout,\n",
    "                prob=prob,\n",
    "                hidden_units=hidden_units\n",
    "            )\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # setting up Loss and Optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(params=model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"Epoch: {epoch}\\n------\")\n",
    "        train_loss, train_accuracy = train_step(\n",
    "                                         model=model,\n",
    "                                         data_loader=train_loader,\n",
    "                                         loss_fn=loss_fn,\n",
    "                                         optimizer=optimizer,\n",
    "                                         accuracy_fn=accuracy_fn,\n",
    "                                         device=device\n",
    "                                     )\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss: .5f} | Train Acc: {train_accuracy: .2f}%\")\n",
    "\n",
    "        val_loss, val_accuracy = test_step(\n",
    "                                    model=model,\n",
    "                                    data_loader=val_loader,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    accuracy_fn=accuracy_fn,\n",
    "                                    device=device\n",
    "                                )\n",
    "\n",
    "        \n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        wandb.log({\"val_accuracy\":val_accuracy, 'val_loss':val_loss, 'train_accuracy':train_accuracy, 'train_loss':train_loss})\n",
    "        \n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de609b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2pf6xnno\n",
      "Sweep URL: https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: csmsgdkp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: ReLU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: Yes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation_fn: ReLU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: half\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: [3, 3, 3, 3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprob: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/wandb/run-20240404_113516-csmsgdkp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/csmsgdkp' target=\"_blank\">stoic-sweep-1</a></strong> to <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/csmsgdkp' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/csmsgdkp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e34bbe998024f97833289f713cc9ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Train Loss:  2.24638 | Train Acc:  18.87%\n",
      "Val Loss: 2.2288, Val Acc: 20.4365\n",
      "Epoch: 1\n",
      "------\n",
      "Train Loss:  2.20926 | Train Acc:  23.14%\n",
      "Val Loss: 2.2077, Val Acc: 23.9087\n",
      "Epoch: 2\n",
      "------\n",
      "Train Loss:  2.19279 | Train Acc:  25.60%\n",
      "Val Loss: 2.1984, Val Acc: 25.0992\n",
      "Epoch: 3\n",
      "------\n",
      "Train Loss:  2.17353 | Train Acc:  27.63%\n",
      "Val Loss: 2.1787, Val Acc: 26.9345\n",
      "Epoch: 4\n",
      "------\n",
      "Train Loss:  2.16815 | Train Acc:  27.85%\n",
      "Val Loss: 2.1897, Val Acc: 25.9425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▄▆██</td></tr><tr><td>train_loss</td><td>█▅▃▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆█▇</td></tr><tr><td>val_loss</td><td>█▅▄▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>27.85242</td></tr><tr><td>train_loss</td><td>2.16815</td></tr><tr><td>val_accuracy</td><td>25.94246</td></tr><tr><td>val_loss</td><td>2.18966</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-sweep-1</strong> at: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/csmsgdkp' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/csmsgdkp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240404_113516-csmsgdkp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hkttlohx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: GELU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: Yes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_activation_fn: SiLU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: Yes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: double\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: [3, 3, 3, 3, 3]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprob: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/wandb/run-20240404_114022-hkttlohx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/hkttlohx' target=\"_blank\">soft-sweep-2</a></strong> to <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/2pf6xnno</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/hkttlohx' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/hkttlohx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c164b02cfaa84ffea7d12ee7827e0b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "------\n",
      "Train Loss:  2.36030 | Train Acc:  9.87%\n",
      "Val Loss: 2.3610, Val Acc: 10.0198\n",
      "Epoch: 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>9.86917</td></tr><tr><td>train_loss</td><td>2.3603</td></tr><tr><td>val_accuracy</td><td>10.01984</td></tr><tr><td>val_loss</td><td>2.36095</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">soft-sweep-2</strong> at: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/hkttlohx' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/hkttlohx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240404_114022-hkttlohx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train():\n",
    "    with wandb.init(project=\"DL_Assignment_2\") as run:\n",
    "        config = wandb.config\n",
    "        train_cnn(train_loader, val_loader, config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_2\")\n",
    "wandb.agent(sweep_id, train, count = 5)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feccbfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
