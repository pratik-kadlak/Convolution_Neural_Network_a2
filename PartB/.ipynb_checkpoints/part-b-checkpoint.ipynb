{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba8454e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075c0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    \"\"\"\n",
    "    Read images from a folder using PyTorch's ImageFolder and DataLoader,\n",
    "    apply transformations, and return NumPy arrays for images and labels.\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "    - X (np.array): NumPy array of images with shape (num_images, channels, height, width).\n",
    "    - y (np.array): NumPy array of labels with shape (num_images,).\n",
    "    \"\"\"\n",
    "    data_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=32) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e662960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):  \n",
    "    \"\"\"\n",
    "    Shuffle data samples and their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): NumPy array containing data samples.\n",
    "    - y (numpy.ndarray): NumPy array containing corresponding labels.\n",
    "\n",
    "    Returns:\n",
    "    - X_shuffled (numpy.ndarray): NumPy array containing shuffled data samples.\n",
    "    - y_shuffled (numpy.ndarray): NumPy array containing corresponding shuffled labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine X, y into a list of tuples\n",
    "    data = list(zip(X, y))\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Unpack the shuffled data back into separate arrays\n",
    "    X_shuffled, y_shuffled = zip(*data)\n",
    "\n",
    "    # Convert the shuffled lists to NumPy arrays \n",
    "    X_shuffled = np.array(X_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "    \n",
    "    return X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66033881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create a PyTorch DataLoader from input data and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.ndarray): Input data array.\n",
    "    - y (numpy.ndarray): Labels array.\n",
    "    - batch_size (int, optional): Batch size for DataLoader (default=32).\n",
    "    - shuffle (bool, optional): Whether to shuffle the data (default=True).\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: PyTorch DataLoader for the input data and labels.\n",
    "    \"\"\"\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # Create a TensorDataset from X_train_tensor and y_train_tensor\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Define batch size and create DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083531f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc458ee7c5a46149e21d565ee6f86aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1001cb31664293a3871c167a1a1e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting the path to dataset\n",
    "train_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/train\"\n",
    "test_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/val\"\n",
    "\n",
    "# reading the images \n",
    "X_train, y_train = read_images(train_path)\n",
    "X_test, y_test = read_images(test_path)\n",
    "\n",
    "# shuffling the data\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "\n",
    "# making data loaders\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eb2f51",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65129b",
   "metadata": {},
   "source": [
    "## GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79964d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tune a pre-trained GoogLeNet model on a custom dataset.\n",
    "\n",
    "This code block loads a pre-trained GoogLeNet model, modifies the final fully connected (FC) layer\n",
    "to have 10 output classes, defines a loss function (CrossEntropyLoss) and optimizer (SGD), and trains\n",
    "the model on a custom dataset using the specified number of epochs.\n",
    "\n",
    "The training loop iterates through each epoch, performing forward and backward passes, and evaluates\n",
    "the model's accuracy on a test dataset after each epoch. The trained model is saved to a file named\n",
    "'googlenet_model.pth' after training.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define GoogLeNet model\n",
    "model = models.googlenet(pretrained=True)  # Load pre-trained weights\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final FC layer\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Access GPU if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for images, labels in train_loader:\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_loader:\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Number of training epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for images, labels in train_loader:  # Iterate over batches of training data\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to device (CPU or GPU)\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        outputs = model(images)  # Forward pass: compute predicted outputs\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Update model parameters based on gradients\n",
    "\n",
    "    # Evaluate the model after each epoch\n",
    "    model.eval()  # Set the model to evaluation mode (disables dropout and batch normalization)\n",
    "    correct = 0  # Initialize number of correctly predicted samples\n",
    "    total = 0  # Initialize total number of samples\n",
    "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
    "        for images, labels in test_loader:  # Iterate over batches of test data\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to device\n",
    "            outputs = model(images)  # Forward pass: compute predicted outputs\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get predicted labels\n",
    "            total += labels.size(0)  # Update total count of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total  # Calculate accuracy percentage\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%')  # Print test accuracy after each epoch\n",
    "\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'googlenet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644bc88",
   "metadata": {},
   "source": [
    "## 1. Freezing all layers except the last layer:\n",
    "- Freeze all layers except the final classification layer.\n",
    "- Fine-tune only the weights of the last layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model using a pre-trained GoogLeNet architecture for a specific classification task.\n",
    "\n",
    "1. Load the pre-trained GoogLeNet model and modify the last layer for the desired number of output classes.\n",
    "2. Freeze all layers except the last layer to only train the new classifier layer.\n",
    "3. Define the optimizer (Adam) and loss function (CrossEntropyLoss).\n",
    "4. Train the model using the specified data loader for a certain number of epochs.\n",
    "\n",
    "Args:\n",
    "- train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "- num_epochs (int): Number of training epochs (default is 10).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# num_classes is the number of classes in your dataset\n",
    "num_classes = 10\n",
    "\n",
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last layer for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes) \n",
    "\n",
    "# Access GPU if available\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train a neural network model using the specified criterion, optimizer, and data loader for a certain number of epochs.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The neural network model to train.\n",
    "    - criterion (torch.nn.Module): The loss function used for optimization.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "    - num_epochs (int): Number of training epochs (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba38f0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d844b68a920148d79866dd343ed89d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9131, Test Accuracy: 0.6975\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    \"\"\"\n",
    "    Test a neural network model using the specified criterion and data loader.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The trained neural network model to evaluate.\n",
    "    - criterion (torch.nn.Module): The loss function used for evaluation.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8179db",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning up to a certain number of layers:\n",
    "\n",
    "- Freeze the initial layers (e.g., convolutional layers) and fine-tune only the later layers (e.g., fully connected layers).\n",
    "- Experiment with different values of 'k' to find the optimal number of layers to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Define the number of layers to fine-tune (k)\n",
    "k = 5  # Example: Fine-tune the last 5 layers\n",
    "\n",
    "# Freeze layers up to k\n",
    "if k > 0:\n",
    "    for i, child in enumerate(model.children()):\n",
    "        if i < k:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Modify the classifier for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = 10  # Change this to your actual number of classes\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Load a pre-trained GoogLeNet model and fine-tune it for a specific classification task.\n",
    "\n",
    "    Parameters:\n",
    "    - model (nn.Module): Pre-trained GoogLeNet model.\n",
    "    - criterion (torch.nn.modules.loss._Loss): Loss function.\n",
    "    - optimizer (torch.optim.optimizer.Optimizer): Optimizer for training.\n",
    "    - train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "    - num_epochs (int): Number of training epochs (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac74a3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17e46e93cdc49ff82be362e43bac3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4563, Test Accuracy: 0.5440\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a trained neural network model using the specified criterion and data loader for test data.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The trained neural network model to evaluate.\n",
    "    - criterion (torch.nn.Module): The loss function used for evaluation.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ee9a",
   "metadata": {},
   "source": [
    "## 3. Feature extraction using pre-trained models:\n",
    "\n",
    "- Use pre-trained models like GoogLeNet, InceptionV3, ResNet50, etc., as feature extractors.\n",
    "- Remove the final classification layer and use the extracted features as inputs to a smaller model (e.g., a simple feedforward neural network).\n",
    "- Train the smaller model on the extracted features to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "716756b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e01b18f57949fcad60bfff2e89fdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Load pre-trained GoogLeNet without the final classification layer\n",
    "googlenet = models.googlenet(pretrained=True).to(device)\n",
    "googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Extract features using GoogLeNet\n",
    "features_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "# Concatenate features and labels\n",
    "features = torch.cat(features_list, dim=0).to(device)\n",
    "labels = torch.cat(labels_list, dim=0).to(device)\n",
    "\n",
    "# Define the input size for the classifier based on the extracted features\n",
    "input_size = features.size(1)\n",
    "\n",
    "# Initialize the simple classifier and move it to the device\n",
    "classifier = SimpleClassifier(input_size, hidden_size=128, num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "585c09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.311309337615967, Accuracy: 10.91%\n",
      "Epoch [2/10], Loss: 2.238849639892578, Accuracy: 26.92%\n",
      "Epoch [3/10], Loss: 2.169893264770508, Accuracy: 37.78%\n",
      "Epoch [4/10], Loss: 2.092411994934082, Accuracy: 46.00%\n",
      "Epoch [5/10], Loss: 2.009640693664551, Accuracy: 52.67%\n",
      "Epoch [6/10], Loss: 1.9256505966186523, Accuracy: 56.91%\n",
      "Epoch [7/10], Loss: 1.8435523509979248, Accuracy: 59.24%\n",
      "Epoch [8/10], Loss: 1.7641732692718506, Accuracy: 60.76%\n",
      "Epoch [9/10], Loss: 1.6886913776397705, Accuracy: 61.57%\n",
      "Epoch [10/10], Loss: 1.6167616844177246, Accuracy: 62.22%\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier on the extracted features\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    accuracy = correct / labels.size(0) * 100\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06c2ebe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727b654a84fd48dfb5099980dc6e53ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 62.30%\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store predicted labels and ground truth labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "classifier.eval()\n",
    "\n",
    "# Iterate over the test_loader\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        features = googlenet(images).squeeze()  # Extract features using GoogLeNet\n",
    "        outputs = classifier(features)  # Get predictions from the classifier\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
    "        predicted_labels.extend(predicted.cpu().numpy())  # Append predicted labels to the list\n",
    "        true_labels.extend(labels.cpu().numpy())  # Append true labels to the list\n",
    "\n",
    "# Convert lists to NumPy arrays for easier analysis\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "print(f'Testing Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e0938",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "- Fine Tuning the Feature Extracted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c72bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path, batch_size):\n",
    "    \"\"\"\n",
    "    Read images from a specified path using PyTorch's DataLoader and apply transformations.\n",
    "\n",
    "    Args:\n",
    "    - path (str): The path to the directory containing the images.\n",
    "    - batch_size (int): The batch size for DataLoader.\n",
    "\n",
    "    Returns:\n",
    "    - X (numpy.ndarray): Array of images.\n",
    "    - y (numpy.ndarray): Array of corresponding labels.\n",
    "    \"\"\"\n",
    "    data_transform = transforms.Compose([transforms.Resize((299,299)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=batch_size) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f58afd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e16d955bf0a4c56802a9cc8a1a9f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185cf486a5f04a6ea3bc3b59b01f7810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/train\"\n",
    "test_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/val\"\n",
    "\n",
    "X_train, y_train = read_images(train_path, 32)\n",
    "X_test, y_test = read_images(test_path, 32)\n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b4b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data():\n",
    "    \"\"\"\n",
    "    Augment data in a DataLoader using various transformations and return an augmented DataLoader.\n",
    "\n",
    "    Args:\n",
    "    - train_loader (DataLoader): DataLoader containing the original training data.\n",
    "\n",
    "    Returns:\n",
    "    - aug_loader (DataLoader): Augmented DataLoader with transformed data for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original train_loader\n",
    "    train_loader_copy = copy.deepcopy(train_loader)\n",
    "    \n",
    "    # Define data augmentation transformations\n",
    "    augmented_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations to the images in train_loader\n",
    "    train_loader_copy.dataset.transform = augmented_transform\n",
    "\n",
    "    augmented_dataset = ConcatDataset([train_loader.dataset, train_loader_copy.dataset])\n",
    "    aug_loader = DataLoader(augmented_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "    return aug_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1fdd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network for classification tasks.\n",
    "\n",
    "    Args:\n",
    "    - input_size (int): The size of the input features.\n",
    "    - activation_func (str): The activation function to use. Options: \"ReLU\", \"SiLU\", \"GELU\", \"Mish\".\n",
    "    - apply_dropout (str): Whether to apply dropout. Options: \"Yes\", \"No\".\n",
    "    - prob (float): Dropout probability.\n",
    "    - hidden_size (int): The size of the hidden layer.\n",
    "    - num_classes (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, input_size, activation_func, apply_dropout, prob, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # trying different activation func\n",
    "        if activation_func == \"ReLU\": self.activation = nn.ReLU()\n",
    "        elif activation_func == \"SiLU\": self.activation = nn.SiLU()\n",
    "        elif activation_func == \"GELU\": self.activation = nn.GELU()\n",
    "        elif activation_func == \"Mish\": self.activation = nn.Mish()\n",
    "               \n",
    "        self.apply_drop = apply_dropout\n",
    "        # Adding Dropout\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.apply_drop == \"Yes\":\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a063890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2baafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    \"\"\"\n",
    "    Extract features using a pre-trained GoogLeNet model without the final classification layer.\n",
    "\n",
    "    Returns:\n",
    "    - googlenet (torch.nn.Module): Pre-trained GoogLeNet model without the final layer.\n",
    "    - features (torch.Tensor): Extracted features from the images.\n",
    "    - labels (torch.Tensor): Corresponding labels for the extracted features.\n",
    "    \"\"\"\n",
    "    # Load pre-trained GoogLeNet without the final classification layer\n",
    "    googlenet = models.googlenet(pretrained=True).to(device)\n",
    "    googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "    # Extract features using GoogLeNet\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "            features_list.append(features)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Concatenate features and labels\n",
    "    features = torch.cat(features_list, dim=0).to(device)\n",
    "    labels = torch.cat(labels_list, dim=0).to(device)\n",
    "    \n",
    "    return googlenet, features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6096c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(googlenet, classifier, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a classifier model using features extracted by a pre-trained GoogLeNet model.\n",
    "\n",
    "    Args:\n",
    "    - googlenet (torch.nn.Module): Pre-trained GoogLeNet model without the final layer.\n",
    "    - classifier (torch.nn.Module): Classifier model to evaluate.\n",
    "    - test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy of the classifier model on the test data.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store predicted labels and ground truth labels\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    classifier.eval()\n",
    "\n",
    "    # Iterate over the test_loader\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Extract features using GoogLeNet\n",
    "            outputs = classifier(features)  # Get predictions from the classifier\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            predicted_labels.extend(predicted.cpu().numpy())  # Append predicted labels to the list\n",
    "            true_labels.extend(labels.cpu().numpy())  # Append true labels to the list\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier analysis\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "    # print(f'Testing Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a54d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"\"\"\n",
    "    Train a simple classifier model using extracted features from a pre-trained GoogLeNet model.\n",
    "\n",
    "    Args:\n",
    "    - config: A configuration object containing hyperparameters and settings for training.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    if config.data_augment == \"Yes\":\n",
    "        data_loader = augment_data()\n",
    "        train_loader = data_loader\n",
    "    \n",
    "    googlenet, features, labels = extract_features()\n",
    "\n",
    "    # Define the input size for the classifier based on the extracted features\n",
    "    input_size = features.size(1)\n",
    "    \n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "    # Initialize the simple classifier and move it to the device\n",
    "    classifier = SimpleClassifier(input_size, config.activation_func, config.dropout, config.prob, config.hidden_units, num_classes=10).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Trying Different Optimizers \n",
    "    if config.optimizer == \"SGD\": optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"Adam\": optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"NAdam\": optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"RMSprop\": optimizer = torch.optim.RMSprop(classifier.parameters(), lr=0.001) \n",
    "        \n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "    # Best Optimizer working is Adam for this problem so trying to change parameters values\n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "    \n",
    "    run_name = f\"epoch_{config.epoch}_opt_{config.optimizer}_act_{config.activation_func}_augment_{config.data_augment}_dropout_{config.dropout}_prob_{config.prob}_hu_{config.hidden_units}\"\n",
    "\n",
    "\n",
    "    # Train the classifier on the extracted features\n",
    "    num_epochs = config.epoch # for 100 epoch this gives accuracy trian_accuracy of 89.52 %\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / labels.size(0) * 100\n",
    "        test_accuracy = evaluate_model(googlenet, classifier, test_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy}')\n",
    "        wandb.log({\"train_accuracy\":train_accuracy, 'train_loss':loss.item(), 'test_accuracy':test_accuracy})\n",
    "        \n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "944485f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "\"name\": \"PartB_FineTuning\",\n",
    "\"metric\": {\n",
    "    \"name\":\"test_accuracy\",\n",
    "    \"goal\": \"maximize\"\n",
    "},\n",
    "\"method\": \"bayes\",\n",
    "\"parameters\": {\n",
    "        \"epoch\": {\n",
    "            \"values\": [10, 20, 30]\n",
    "        },\n",
    "        \"activation_func\": {\n",
    "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
    "        },\n",
    "        \"data_augment\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"prob\": {\n",
    "            \"values\": [0.2, 0.3]\n",
    "        },\n",
    "        \"hidden_units\": {\n",
    "            \"values\": [256, 512, 1024]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"SGD\", \"Adam\", \"NAdam\", \"RMSprop\"]\n",
    "        }\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "828b7204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5rf38dbi\n",
      "Sweep URL: https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/5rf38dbi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3usaecgk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: GELU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: Yes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprob: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkadlakpratik\u001b[0m (\u001b[33mspace_monkeys\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/Convolution-Neural-Network/PartB/wandb/run-20240407_163216-3usaecgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/3usaecgk' target=\"_blank\">solar-sweep-1</a></strong> to <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/5rf38dbi' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/5rf38dbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/5rf38dbi' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/5rf38dbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/3usaecgk' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/3usaecgk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratikkadlak/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/pratikkadlak/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6353329733f94a758fc7252570fcf27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e58a420aefb4306bc5b6081fdcb3a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.315261125564575, Accuracy: 8.96%, Test Accuracy: 23.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6959cf22f48a4a38bace49985440ec96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 2.509401798248291, Accuracy: 22.09%, Test Accuracy: 31.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e973708ac8e54bf8a6e6a76dc749efb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 2.740562677383423, Accuracy: 24.01%, Test Accuracy: 10.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b6d534923c4b17b1b91a58cb58e223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 6.202073097229004, Accuracy: 11.86%, Test Accuracy: 20.849999999999998\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5c4dc4da2448888538e0e7a71ff701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 5.133575916290283, Accuracy: 17.11%, Test Accuracy: 19.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ed3f5a967a428cbfd58470582107db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 3.2188847064971924, Accuracy: 18.85%, Test Accuracy: 28.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0beea8e800407fac46cdf20d713ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 2.477202892303467, Accuracy: 24.66%, Test Accuracy: 30.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fc26ef66624b1881c8a5c5336ff3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 2.066556930541992, Accuracy: 28.31%, Test Accuracy: 48.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5b979138d84f018a85d804625e7db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 1.8021153211593628, Accuracy: 40.21%, Test Accuracy: 59.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb065f244244fb3846ae0c9093500f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 1.610175609588623, Accuracy: 49.86%, Test Accuracy: 60.050000000000004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4db55f504a34b7ca07a3706d3400ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 1.5725457668304443, Accuracy: 50.87%, Test Accuracy: 55.85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d091fd5b15f41b483185425ab91f18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 1.5699949264526367, Accuracy: 51.04%, Test Accuracy: 50.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fd9e703a0947e3ace13e1c344f5d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 1.6169414520263672, Accuracy: 46.55%, Test Accuracy: 51.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f628bd2b69e4bceae58cfe4f5716616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 1.615235686302185, Accuracy: 47.62%, Test Accuracy: 53.400000000000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf22d4da6ccb409e820762bd3e9d83c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 1.5598338842391968, Accuracy: 50.05%, Test Accuracy: 61.550000000000004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8ff76c5eaa4f2f91ee7e05fbbf56ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 1.4394842386245728, Accuracy: 56.96%, Test Accuracy: 61.75000000000001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456d7c0b36aa479fab70eb3dfec6aa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 1.4018408060073853, Accuracy: 57.88%, Test Accuracy: 63.65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f146668dc8c42d9aaf04302caa8ddcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 1.3793177604675293, Accuracy: 60.26%, Test Accuracy: 58.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7725e0938b44deeb0e23eaa02aab955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 1.3815555572509766, Accuracy: 56.46%, Test Accuracy: 61.150000000000006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f14f3023c4647b473ae985e3e687c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 1.3974430561065674, Accuracy: 57.94%, Test Accuracy: 57.99999999999999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>▃▄▁▂▂▃▄▆▇█▇▆▆▇███▇█▇</td></tr><tr><td>train_accuracy</td><td>▁▃▃▁▂▂▃▄▅▇▇▇▆▆▇███▇█</td></tr><tr><td>train_loss</td><td>▂▃▃█▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>58.0</td></tr><tr><td>train_accuracy</td><td>57.93579</td></tr><tr><td>train_loss</td><td>1.39744</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-sweep-1</strong> at: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/3usaecgk' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/3usaecgk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240407_163216-3usaecgk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Initialize a Weights & Biases run and train a CNN model using the configured hyperparameters.\n",
    "\n",
    "    Uses the `wandb.sweep` function to create a sweep with the specified configuration,\n",
    "    then runs the training process using the `wandb.agent` function.\n",
    "    \"\"\"\n",
    "    with wandb.init(project=\"DL_Assignment_2\") as run:\n",
    "        config = wandb.config\n",
    "        train_model(config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_2\")\n",
    "wandb.agent(sweep_id, train, count = 1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832f01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
