{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:32.417232Z",
     "iopub.status.busy": "2024-04-06T12:17:32.416604Z",
     "iopub.status.idle": "2024-04-06T12:17:39.465618Z",
     "shell.execute_reply": "2024-04-06T12:17:39.464623Z",
     "shell.execute_reply.started": "2024-04-06T12:17:32.417199Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:41.943297Z",
     "iopub.status.busy": "2024-04-06T12:17:41.942059Z",
     "iopub.status.idle": "2024-04-06T12:17:41.949571Z",
     "shell.execute_reply": "2024-04-06T12:17:41.948612Z",
     "shell.execute_reply.started": "2024-04-06T12:17:41.943263Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    data_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=32) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:42.539040Z",
     "iopub.status.busy": "2024-04-06T12:17:42.538672Z",
     "iopub.status.idle": "2024-04-06T12:17:42.546912Z",
     "shell.execute_reply": "2024-04-06T12:17:42.546006Z",
     "shell.execute_reply.started": "2024-04-06T12:17:42.539012Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split(train_size:float = 0.2):    \n",
    "    # Initialize lists to store validation and training data\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # 1000 because we have 1000 images of each class\n",
    "    samples_per_class_val = (int) (1000 * train_size)\n",
    "    \n",
    "    for class_label in range(10):\n",
    "        # extract indices corresponding to the current class\n",
    "        class_indices = np.where(y_train==class_label)[0]\n",
    "        \n",
    "        # randomly select sample_per_class_val indices for validation\n",
    "        val_indices = np.random.choice(class_indices, samples_per_class_val, replace=False)\n",
    "        \n",
    "        # append the selected val data to X_val and y_val\n",
    "        X_val.extend(X_train[val_indices])\n",
    "        y_val.extend(y_train[val_indices])\n",
    "        \n",
    "        # append the remaining data to X_train and y_train\n",
    "        train_indices = np.setdiff1d(class_indices, val_indices)\n",
    "        X.extend(X_train[train_indices])\n",
    "        y.extend(y_train[train_indices])\n",
    "\n",
    "    # convert python lists to np array\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, X_val, y, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:42.949115Z",
     "iopub.status.busy": "2024-04-06T12:17:42.948805Z",
     "iopub.status.idle": "2024-04-06T12:17:42.955301Z",
     "shell.execute_reply": "2024-04-06T12:17:42.954450Z",
     "shell.execute_reply.started": "2024-04-06T12:17:42.949089Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):    \n",
    "    # Combine X, y into a list of tuples\n",
    "    data = list(zip(X, y))\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Unpack the shuffled data back into separate arrays\n",
    "    X_shuffled, y_shuffled = zip(*data)\n",
    "\n",
    "    # Convert the shuffled lists to NumPy arrays \n",
    "    X_shuffled = np.array(X_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "    \n",
    "    return X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:43.434041Z",
     "iopub.status.busy": "2024-04-06T12:17:43.433633Z",
     "iopub.status.idle": "2024-04-06T12:17:43.443826Z",
     "shell.execute_reply": "2024-04-06T12:17:43.442744Z",
     "shell.execute_reply.started": "2024-04-06T12:17:43.434012Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # Create a TensorDataset from X_train_tensor and y_train_tensor\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Define batch size and create DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T07:55:49.677789Z",
     "iopub.status.busy": "2024-04-06T07:55:49.676956Z",
     "iopub.status.idle": "2024-04-06T08:00:47.045550Z",
     "shell.execute_reply": "2024-04-06T08:00:47.044190Z",
     "shell.execute_reply.started": "2024-04-06T07:55:49.677755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907baba8e1b442e1b549827b64d7ba5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27123f87a6134cadb407136a4ee51de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"/kaggle/input/inaturalist/inaturalist_12K/train\"\n",
    "test_path = \"/kaggle/input/inaturalist/inaturalist_12K/val\"\n",
    "\n",
    "X_train, y_train = read_images(train_path)\n",
    "X_test, y_test = read_images(test_path)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_val_split(0.2) \n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "X_val, y_val = shuffle_data(X_val, y_val)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "val_loader = create_dataloader(X_val, y_val, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:13:39.572853Z",
     "iopub.status.busy": "2024-04-05T18:13:39.572490Z",
     "iopub.status.idle": "2024-04-05T18:19:01.583067Z",
     "shell.execute_reply": "2024-04-05T18:19:01.582077Z",
     "shell.execute_reply.started": "2024-04-05T18:13:39.572823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Accuracy: 48.25%\n",
      "Epoch 2/10, Test Accuracy: 56.50%\n",
      "Epoch 3/10, Test Accuracy: 54.45%\n",
      "Epoch 4/10, Test Accuracy: 51.75%\n",
      "Epoch 5/10, Test Accuracy: 56.75%\n",
      "Epoch 6/10, Test Accuracy: 57.70%\n",
      "Epoch 7/10, Test Accuracy: 57.45%\n",
      "Epoch 8/10, Test Accuracy: 61.45%\n",
      "Epoch 9/10, Test Accuracy: 58.80%\n",
      "Epoch 10/10, Test Accuracy: 57.25%\n"
     ]
    }
   ],
   "source": [
    "# Define GoogLeNet model\n",
    "model = models.googlenet(pretrained=True)  # Load pre-trained weights\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final FC layer\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'googlenet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all layers except the last layer:\n",
    "- Freeze all layers except the final classification layer.\n",
    "- Fine-tune only the weights of the last layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:29:23.540396Z",
     "iopub.status.busy": "2024-04-05T18:29:23.539768Z",
     "iopub.status.idle": "2024-04-05T18:31:06.451193Z",
     "shell.execute_reply": "2024-04-05T18:31:06.450189Z",
     "shell.execute_reply.started": "2024-04-05T18:29:23.540360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b554a21ba7e4410be7879d45f977bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4414, Accuracy: 0.5614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846293acdaf84278a21aa16be9c0c26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0779, Accuracy: 0.6502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a04e0e0d0704e2985ee8e0cc131830d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0010, Accuracy: 0.6740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ed27295bd54e378f1bdc0923eb2f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9677, Accuracy: 0.6835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b1276e64b54742b1f489cd887b87ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9469, Accuracy: 0.6881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0693a7c545a24e778209e6c006c7b381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.9390, Accuracy: 0.6882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0232a000ae21487b8a7b309ebc7da20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.9314, Accuracy: 0.6906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339157189af54f35b1e8ae40fc4dd68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.9120, Accuracy: 0.6978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2252e6a631e04357952955aa2d61add9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.9109, Accuracy: 0.6973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e6baacd33b4bf5af67e324aca45120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.8974, Accuracy: 0.7040\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last layer for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)  # num_classes is the number of classes in your dataset\n",
    "\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for images, labels in tqdm(train_loader):  # assuming you have a DataLoader for training data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:31:55.986266Z",
     "iopub.status.busy": "2024-04-05T18:31:55.985894Z",
     "iopub.status.idle": "2024-04-05T18:31:58.517918Z",
     "shell.execute_reply": "2024-04-05T18:31:58.516966Z",
     "shell.execute_reply.started": "2024-04-05T18:31:55.986236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39920b9ef1c241d5bc7fdbd1aff18008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8675, Test Accuracy: 0.7150\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning up to a certain number of layers:\n",
    "\n",
    "- Freeze the initial layers (e.g., convolutional layers) and fine-tune only the later layers (e.g., fully connected layers).\n",
    "- Experiment with different values of 'k' to find the optimal number of layers to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:32:27.774829Z",
     "iopub.status.busy": "2024-04-05T18:32:27.774496Z",
     "iopub.status.idle": "2024-04-05T18:36:26.934289Z",
     "shell.execute_reply": "2024-04-05T18:36:26.933358Z",
     "shell.execute_reply.started": "2024-04-05T18:32:27.774800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1858c72d340a4f99ae94b4c417373eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3999, Accuracy: 0.5202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ebfd54c07045a59433e9bd7b469b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0505, Accuracy: 0.6426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3ba61d74234223b23cc9f7bba211d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8365, Accuracy: 0.7170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67d70d9b6884eef9363f3542d149e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.6334, Accuracy: 0.7886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dd8c1bb5f94a4cbb8bb1f9ecc56385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.5255, Accuracy: 0.8207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d142c8ad54812869708b6e0f97fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.4238, Accuracy: 0.8595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d0d1bbcfc64f5187246a44fd22d14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.3228, Accuracy: 0.8946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73af7210c0e241c5a5bfed766433ade6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.2931, Accuracy: 0.9035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9111e97e68c422dabdc0aa74bd31133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.2604, Accuracy: 0.9149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd731bf030a445aa7f09ea3f3cad16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.1855, Accuracy: 0.9374\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Define the number of layers to fine-tune (k)\n",
    "k = 5  # Example: Fine-tune the last 5 layers\n",
    "\n",
    "# Freeze layers up to k\n",
    "if k > 0:\n",
    "    for i, child in enumerate(model.children()):\n",
    "        if i < k:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Modify the classifier for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = 10  # Change this to your actual number of classes\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:36:40.260613Z",
     "iopub.status.busy": "2024-04-05T18:36:40.260239Z",
     "iopub.status.idle": "2024-04-05T18:36:42.785710Z",
     "shell.execute_reply": "2024-04-05T18:36:42.784783Z",
     "shell.execute_reply.started": "2024-04-05T18:36:40.260586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cecaa477b904fb294d4cf1382b0ee4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7242, Test Accuracy: 0.6265\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using pre-trained models:\n",
    "\n",
    "- Use pre-trained models like GoogLeNet, InceptionV3, ResNet50, etc., as feature extractors.\n",
    "- Remove the final classification layer and use the extracted features as inputs to a smaller model (e.g., a simple feedforward neural network).\n",
    "- Train the smaller model on the extracted features to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T06:17:34.458911Z",
     "iopub.status.busy": "2024-04-06T06:17:34.458546Z",
     "iopub.status.idle": "2024-04-06T06:17:44.606636Z",
     "shell.execute_reply": "2024-04-06T06:17:44.605685Z",
     "shell.execute_reply.started": "2024-04-06T06:17:34.458883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1af2e08374938adf87c096b08aab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3161704540252686\n",
      "Epoch [2/10], Loss: 2.24755859375\n",
      "Epoch [3/10], Loss: 2.186673164367676\n",
      "Epoch [4/10], Loss: 2.1165056228637695\n",
      "Epoch [5/10], Loss: 2.0432612895965576\n",
      "Epoch [6/10], Loss: 1.9699397087097168\n",
      "Epoch [7/10], Loss: 1.895399570465088\n",
      "Epoch [8/10], Loss: 1.8205984830856323\n",
      "Epoch [9/10], Loss: 1.7479304075241089\n",
      "Epoch [10/10], Loss: 1.6778099536895752\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pre-trained GoogLeNet without the final classification layer\n",
    "googlenet = models.googlenet(pretrained=True).to(device)\n",
    "googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Extract features using GoogLeNet\n",
    "features_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "# Concatenate features and labels\n",
    "features = torch.cat(features_list, dim=0).to(device)\n",
    "labels = torch.cat(labels_list, dim=0).to(device)\n",
    "\n",
    "# Define the input size for the classifier based on the extracted features\n",
    "input_size = features.size(1)\n",
    "\n",
    "# Initialize the simple classifier and move it to the device\n",
    "classifier = SimpleClassifier(input_size, hidden_size=128, num_classes=10).to(device)\n",
    "# classifier = SimpleClassifier(input_size, hidden_size=128, num_classes=10)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier on the extracted features\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save or use the trained classifier for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- fine tuning the feature extracted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:56.301385Z",
     "iopub.status.busy": "2024-04-06T12:17:56.300732Z",
     "iopub.status.idle": "2024-04-06T12:17:56.307838Z",
     "shell.execute_reply": "2024-04-06T12:17:56.306859Z",
     "shell.execute_reply.started": "2024-04-06T12:17:56.301356Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_images(path, batch_size):\n",
    "    data_transform = transforms.Compose([transforms.Resize((299,299)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=batch_size) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124408cef2a74a509e30017161c91f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b58a47b03e04153a7333e45386c4edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/train\"\n",
    "test_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/val\"\n",
    "\n",
    "X_train, y_train = read_images(train_path, 32)\n",
    "X_test, y_test = read_images(test_path, 32)\n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data():\n",
    "    \"\"\"\n",
    "    Augment data in a DataLoader using various transformations and return an augmented DataLoader.\n",
    "\n",
    "    Args:\n",
    "    - train_loader (DataLoader): DataLoader containing the original training data.\n",
    "\n",
    "    Returns:\n",
    "    - aug_loader (DataLoader): Augmented DataLoader with transformed data for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original train_loader\n",
    "    train_loader_copy = copy.deepcopy(train_loader)\n",
    "    \n",
    "    # Define data augmentation transformations\n",
    "    augmented_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations to the images in train_loader\n",
    "    train_loader_copy.dataset.transform = augmented_transform\n",
    "\n",
    "    augmented_dataset = ConcatDataset([train_loader.dataset, train_loader_copy.dataset])\n",
    "    aug_loader = DataLoader(augmented_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "    return aug_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:22:33.563289Z",
     "iopub.status.busy": "2024-04-06T12:22:33.562926Z",
     "iopub.status.idle": "2024-04-06T12:22:33.571837Z",
     "shell.execute_reply": "2024-04-06T12:22:33.570843Z",
     "shell.execute_reply.started": "2024-04-06T12:22:33.563259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, activation_func, apply_dropout, prob, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # trying different activation func\n",
    "        if activation_func == \"ReLU\": self.activation = nn.ReLU()\n",
    "        elif activation_func == \"SiLU\": self.activation = nn.SiLU()\n",
    "        elif activation_func == \"GELU\": self.activation = nn.GELU()\n",
    "        elif activation_func == \"Mish\": self.activation = nn.Mish()\n",
    "                \n",
    "        # Adding Dropout\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if apply_dropout == \"Yes\":\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:38:55.131403Z",
     "iopub.status.busy": "2024-04-06T12:38:55.131074Z",
     "iopub.status.idle": "2024-04-06T12:38:55.150496Z",
     "shell.execute_reply": "2024-04-06T12:38:55.149418Z",
     "shell.execute_reply.started": "2024-04-06T12:38:55.131378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def extract_features():\n",
    "    # Load pre-trained GoogLeNet without the final classification layer\n",
    "    googlenet = models.googlenet(pretrained=True).to(device)\n",
    "    googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "    # Extract features using GoogLeNet\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "            features_list.append(features)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Concatenate features and labels\n",
    "    features = torch.cat(features_list, dim=0).to(device)\n",
    "    labels = torch.cat(labels_list, dim=0).to(device)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def evaluate_model(classifier, test_loader):\n",
    "    # Initialize lists to store predicted labels and ground truth labels\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    classifier.eval()\n",
    "\n",
    "    # Iterate over the test_loader\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Extract features using GoogLeNet\n",
    "            outputs = classifier(features)  # Get predictions from the classifier\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            predicted_labels.extend(predicted.cpu().numpy())  # Append predicted labels to the list\n",
    "            true_labels.extend(labels.cpu().numpy())  # Append true labels to the list\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier analysis\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "    # print(f'Testing Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def train_model(config):\n",
    "    if config.data_augment == \"Yes\":\n",
    "        data_loader = augment_data()\n",
    "        train_loader = data_loader\n",
    "    \n",
    "    features, labels = extract_features()\n",
    "\n",
    "    # Define the input size for the classifier based on the extracted features\n",
    "    input_size = features.size(1)\n",
    "    \n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "    # Initialize the simple classifier and move it to the device\n",
    "    classifier = SimpleClassifier(input_size, config.activation_func, config.dropout, config.prob, config.hidden_units, num_classes=10).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Trying Different Optimizers \n",
    "    if config.optimizer == \"SGD\": optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"Adam\": optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"NAdam\": optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"RMSprop\": optimizer = torch.optim.RMSprop(classifier.parameters(), lr=0.001) \n",
    "        \n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "    # Best Optimizer working is Adam for this problem so trying to change parameters values\n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "    \n",
    "    run_name = f\"epoch_{config.epoch}_opt_{config.optimizer}_act_{config.activation_func}_augment_{config.data_augment}_dropout_{config.dropout}_prob_{config.prob}_hu_{config.hidden_units}\"\n",
    "\n",
    "\n",
    "    # Train the classifier on the extracted features\n",
    "    num_epochs = config.epoch # for 100 epoch this gives accuracy trian_accuracy of 89.52 %\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / labels.size(0) * 100\n",
    "        test_accuracy = evaluate_model(classifier, test_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy}')\n",
    "        wandb.log({\"train_accuracy\":train_accuracy, 'train_loss':loss.item(), 'test_accuracy':test_accuracy})\n",
    "        \n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:39:01.543966Z",
     "iopub.status.busy": "2024-04-06T12:39:01.543269Z",
     "iopub.status.idle": "2024-04-06T12:39:01.550515Z",
     "shell.execute_reply": "2024-04-06T12:39:01.549351Z",
     "shell.execute_reply.started": "2024-04-06T12:39:01.543930Z"
    }
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "\"name\": \"PartB_FineTuning\",\n",
    "\"metric\": {\n",
    "    \"name\":\"test_accuracy\",\n",
    "    \"goal\": \"maximize\"\n",
    "},\n",
    "\"method\": \"bayes\",\n",
    "\"parameters\": {\n",
    "        \"epoch\": {\n",
    "            \"values\": [10, 20, 30]\n",
    "        },\n",
    "        \"activation_func\": {\n",
    "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
    "        },\n",
    "        \"data_augment\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"prob\": {\n",
    "            \"values\": [0.2, 0.3]\n",
    "        },\n",
    "        \"hidden_units\": {\n",
    "            \"values\": [256, 512, 1024]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"SGD\", \"Adam\", \"NAdam\", \"RMSprop\"]\n",
    "        }\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:39:02.259261Z",
     "iopub.status.busy": "2024-04-06T12:39:02.258694Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ayut5n8x\n",
      "Sweep URL: https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/ayut5n8x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ZMQDisplayPublisher' object has no attribute '_orig_publish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39msweep(sweep_config, project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDL_Assignment_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, train, count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:4104\u001b[0m, in \u001b[0;36mfinish\u001b[0;34m(exit_code, quiet)\u001b[0m\n\u001b[1;32m   4094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   4095\u001b[0m \n\u001b[1;32m   4096\u001b[0m \u001b[38;5;124;03mThis is used when creating multiple runs in the same process.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4101\u001b[0m \u001b[38;5;124;03m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   4102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mrun:\n\u001b[0;32m-> 4104\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mfinish(exit_code\u001b[38;5;241m=\u001b[39mexit_code, quiet\u001b[38;5;241m=\u001b[39mquiet)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:420\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:361\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:1961\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_noop\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;129m@_run_decorator\u001b[39m\u001b[38;5;241m.\u001b[39m_attach\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfinish\u001b[39m(\n\u001b[1;32m   1950\u001b[0m     \u001b[38;5;28mself\u001b[39m, exit_code: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, quiet: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1951\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \n\u001b[1;32m   1954\u001b[0m \u001b[38;5;124;03m    This is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;124;03m        quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(exit_code, quiet)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_run.py:1974\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n\u001b[1;32m   1973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook\u001b[38;5;241m.\u001b[39mstage \u001b[38;5;241m==\u001b[39m TeardownStage\u001b[38;5;241m.\u001b[39mEARLY:\n\u001b[0;32m-> 1974\u001b[0m         hook\u001b[38;5;241m.\u001b[39mcall()\n\u001b[1;32m   1976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atexit_cleanup(exit_code\u001b[38;5;241m=\u001b[39mexit_code)\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39m_global_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wandb/sdk/wandb_init.py:462\u001b[0m, in \u001b[0;36m_WandbInit._jupyter_teardown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_pause_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hook\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m:\n\u001b[1;32m    461\u001b[0m         ipython\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39munregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_run_cell\u001b[39m\u001b[38;5;124m\"\u001b[39m, hook)\n\u001b[0;32m--> 462\u001b[0m ipython\u001b[38;5;241m.\u001b[39mdisplay_pub\u001b[38;5;241m.\u001b[39mpublish \u001b[38;5;241m=\u001b[39m ipython\u001b[38;5;241m.\u001b[39mdisplay_pub\u001b[38;5;241m.\u001b[39m_orig_publish\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m ipython\u001b[38;5;241m.\u001b[39mdisplay_pub\u001b[38;5;241m.\u001b[39m_orig_publish\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Initialize a Weights & Biases run and train a CNN model using the configured hyperparameters.\n",
    "\n",
    "    Uses the `wandb.sweep` function to create a sweep with the specified configuration,\n",
    "    then runs the training process using the `wandb.agent` function.\n",
    "    \"\"\"\n",
    "    with wandb.init(project=\"DL_Assignment_2\") as run:\n",
    "        config = wandb.config\n",
    "        train_model(config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_2\")\n",
    "wandb.agent(sweep_id, train, count = 50)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:58:22.199187Z",
     "iopub.status.busy": "2024-04-06T11:58:22.198831Z",
     "iopub.status.idle": "2024-04-06T11:58:23.249815Z",
     "shell.execute_reply": "2024-04-06T11:58:23.248948Z",
     "shell.execute_reply.started": "2024-04-06T11:58:22.199160Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4722659,
     "sourceId": 8015804,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
