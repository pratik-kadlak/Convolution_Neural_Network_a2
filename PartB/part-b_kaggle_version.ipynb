{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:32.417232Z",
     "iopub.status.busy": "2024-04-06T12:17:32.416604Z",
     "iopub.status.idle": "2024-04-06T12:17:39.465618Z",
     "shell.execute_reply": "2024-04-06T12:17:39.464623Z",
     "shell.execute_reply.started": "2024-04-06T12:17:32.417199Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:41.943297Z",
     "iopub.status.busy": "2024-04-06T12:17:41.942059Z",
     "iopub.status.idle": "2024-04-06T12:17:41.949571Z",
     "shell.execute_reply": "2024-04-06T12:17:41.948612Z",
     "shell.execute_reply.started": "2024-04-06T12:17:41.943263Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_images(path):\n",
    "    data_transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=32) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:42.539040Z",
     "iopub.status.busy": "2024-04-06T12:17:42.538672Z",
     "iopub.status.idle": "2024-04-06T12:17:42.546912Z",
     "shell.execute_reply": "2024-04-06T12:17:42.546006Z",
     "shell.execute_reply.started": "2024-04-06T12:17:42.539012Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_split(train_size:float = 0.2):    \n",
    "    # Initialize lists to store validation and training data\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # 1000 because we have 1000 images of each class\n",
    "    samples_per_class_val = (int) (1000 * train_size)\n",
    "    \n",
    "    for class_label in range(10):\n",
    "        # extract indices corresponding to the current class\n",
    "        class_indices = np.where(y_train==class_label)[0]\n",
    "        \n",
    "        # randomly select sample_per_class_val indices for validation\n",
    "        val_indices = np.random.choice(class_indices, samples_per_class_val, replace=False)\n",
    "        \n",
    "        # append the selected val data to X_val and y_val\n",
    "        X_val.extend(X_train[val_indices])\n",
    "        y_val.extend(y_train[val_indices])\n",
    "        \n",
    "        # append the remaining data to X_train and y_train\n",
    "        train_indices = np.setdiff1d(class_indices, val_indices)\n",
    "        X.extend(X_train[train_indices])\n",
    "        y.extend(y_train[train_indices])\n",
    "\n",
    "    # convert python lists to np array\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    return X, X_val, y, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:42.949115Z",
     "iopub.status.busy": "2024-04-06T12:17:42.948805Z",
     "iopub.status.idle": "2024-04-06T12:17:42.955301Z",
     "shell.execute_reply": "2024-04-06T12:17:42.954450Z",
     "shell.execute_reply.started": "2024-04-06T12:17:42.949089Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X, y):    \n",
    "    # Combine X, y into a list of tuples\n",
    "    data = list(zip(X, y))\n",
    "\n",
    "    # Shuffle the combined data\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # Unpack the shuffled data back into separate arrays\n",
    "    X_shuffled, y_shuffled = zip(*data)\n",
    "\n",
    "    # Convert the shuffled lists to NumPy arrays \n",
    "    X_shuffled = np.array(X_shuffled)\n",
    "    y_shuffled = np.array(y_shuffled)\n",
    "    \n",
    "    return X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:43.434041Z",
     "iopub.status.busy": "2024-04-06T12:17:43.433633Z",
     "iopub.status.idle": "2024-04-06T12:17:43.443826Z",
     "shell.execute_reply": "2024-04-06T12:17:43.442744Z",
     "shell.execute_reply.started": "2024-04-06T12:17:43.434012Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloader(X, y, batch_size, shuffle=True):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_tensor = torch.from_numpy(X)\n",
    "    y_tensor = torch.from_numpy(y)\n",
    "\n",
    "    # Create a TensorDataset from X_train_tensor and y_train_tensor\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "    # Define batch size and create DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T07:55:49.677789Z",
     "iopub.status.busy": "2024-04-06T07:55:49.676956Z",
     "iopub.status.idle": "2024-04-06T08:00:47.045550Z",
     "shell.execute_reply": "2024-04-06T08:00:47.044190Z",
     "shell.execute_reply.started": "2024-04-06T07:55:49.677755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907baba8e1b442e1b549827b64d7ba5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27123f87a6134cadb407136a4ee51de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"/kaggle/input/inaturalist/inaturalist_12K/train\"\n",
    "test_path = \"/kaggle/input/inaturalist/inaturalist_12K/val\"\n",
    "\n",
    "X_train, y_train = read_images(train_path)\n",
    "X_test, y_test = read_images(test_path)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_val_split(0.2) \n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "X_val, y_val = shuffle_data(X_val, y_val)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "val_loader = create_dataloader(X_val, y_val, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:13:39.572853Z",
     "iopub.status.busy": "2024-04-05T18:13:39.572490Z",
     "iopub.status.idle": "2024-04-05T18:19:01.583067Z",
     "shell.execute_reply": "2024-04-05T18:19:01.582077Z",
     "shell.execute_reply.started": "2024-04-05T18:13:39.572823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Test Accuracy: 48.25%\n",
      "Epoch 2/10, Test Accuracy: 56.50%\n",
      "Epoch 3/10, Test Accuracy: 54.45%\n",
      "Epoch 4/10, Test Accuracy: 51.75%\n",
      "Epoch 5/10, Test Accuracy: 56.75%\n",
      "Epoch 6/10, Test Accuracy: 57.70%\n",
      "Epoch 7/10, Test Accuracy: 57.45%\n",
      "Epoch 8/10, Test Accuracy: 61.45%\n",
      "Epoch 9/10, Test Accuracy: 58.80%\n",
      "Epoch 10/10, Test Accuracy: 57.25%\n"
     ]
    }
   ],
   "source": [
    "# Define GoogLeNet model\n",
    "model = models.googlenet(pretrained=True)  # Load pre-trained weights\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # Modify final FC layer\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'googlenet_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing all layers except the last layer:\n",
    "- Freeze all layers except the final classification layer.\n",
    "- Fine-tune only the weights of the last layer during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:29:23.540396Z",
     "iopub.status.busy": "2024-04-05T18:29:23.539768Z",
     "iopub.status.idle": "2024-04-05T18:31:06.451193Z",
     "shell.execute_reply": "2024-04-05T18:31:06.450189Z",
     "shell.execute_reply.started": "2024-04-05T18:29:23.540360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b554a21ba7e4410be7879d45f977bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.4414, Accuracy: 0.5614\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846293acdaf84278a21aa16be9c0c26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0779, Accuracy: 0.6502\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a04e0e0d0704e2985ee8e0cc131830d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0010, Accuracy: 0.6740\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ed27295bd54e378f1bdc0923eb2f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9677, Accuracy: 0.6835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b1276e64b54742b1f489cd887b87ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9469, Accuracy: 0.6881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0693a7c545a24e778209e6c006c7b381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.9390, Accuracy: 0.6882\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0232a000ae21487b8a7b309ebc7da20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.9314, Accuracy: 0.6906\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339157189af54f35b1e8ae40fc4dd68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.9120, Accuracy: 0.6978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2252e6a631e04357952955aa2d61add9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.9109, Accuracy: 0.6973\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e6baacd33b4bf5af67e324aca45120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.8974, Accuracy: 0.7040\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Freeze all layers except the last layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last layer for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)  # num_classes is the number of classes in your dataset\n",
    "\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for images, labels in tqdm(train_loader):  # assuming you have a DataLoader for training data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_predictions / total_predictions\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:31:55.986266Z",
     "iopub.status.busy": "2024-04-05T18:31:55.985894Z",
     "iopub.status.idle": "2024-04-05T18:31:58.517918Z",
     "shell.execute_reply": "2024-04-05T18:31:58.516966Z",
     "shell.execute_reply.started": "2024-04-05T18:31:55.986236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39920b9ef1c241d5bc7fdbd1aff18008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8675, Test Accuracy: 0.7150\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning up to a certain number of layers:\n",
    "\n",
    "- Freeze the initial layers (e.g., convolutional layers) and fine-tune only the later layers (e.g., fully connected layers).\n",
    "- Experiment with different values of 'k' to find the optimal number of layers to fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:32:27.774829Z",
     "iopub.status.busy": "2024-04-05T18:32:27.774496Z",
     "iopub.status.idle": "2024-04-05T18:36:26.934289Z",
     "shell.execute_reply": "2024-04-05T18:36:26.933358Z",
     "shell.execute_reply.started": "2024-04-05T18:32:27.774800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1858c72d340a4f99ae94b4c417373eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.3999, Accuracy: 0.5202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ebfd54c07045a59433e9bd7b469b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.0505, Accuracy: 0.6426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3ba61d74234223b23cc9f7bba211d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8365, Accuracy: 0.7170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67d70d9b6884eef9363f3542d149e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.6334, Accuracy: 0.7886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92dd8c1bb5f94a4cbb8bb1f9ecc56385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.5255, Accuracy: 0.8207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3d142c8ad54812869708b6e0f97fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.4238, Accuracy: 0.8595\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d0d1bbcfc64f5187246a44fd22d14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.3228, Accuracy: 0.8946\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73af7210c0e241c5a5bfed766433ade6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.2931, Accuracy: 0.9035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9111e97e68c422dabdc0aa74bd31133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.2604, Accuracy: 0.9149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd731bf030a445aa7f09ea3f3cad16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.1855, Accuracy: 0.9374\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GoogLeNet\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "# Define the number of layers to fine-tune (k)\n",
    "k = 5  # Example: Fine-tune the last 5 layers\n",
    "\n",
    "# Freeze layers up to k\n",
    "if k > 0:\n",
    "    for i, child in enumerate(model.children()):\n",
    "        if i < k:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Modify the classifier for your specific classification task\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = 10  # Change this to your actual number of classes\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, criterion, optimizer, train_loader, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-05T18:36:40.260613Z",
     "iopub.status.busy": "2024-04-05T18:36:40.260239Z",
     "iopub.status.idle": "2024-04-05T18:36:42.785710Z",
     "shell.execute_reply": "2024-04-05T18:36:42.784783Z",
     "shell.execute_reply.started": "2024-04-05T18:36:40.260586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cecaa477b904fb294d4cf1382b0ee4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7242, Test Accuracy: 0.6265\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):  # assuming you have a DataLoader for test data\n",
    "            images , labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct_predictions / total_predictions\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "test_model(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction using pre-trained models:\n",
    "\n",
    "- Use pre-trained models like GoogLeNet, InceptionV3, ResNet50, etc., as feature extractors.\n",
    "- Remove the final classification layer and use the extracted features as inputs to a smaller model (e.g., a simple feedforward neural network).\n",
    "- Train the smaller model on the extracted features to classify images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T06:17:34.458911Z",
     "iopub.status.busy": "2024-04-06T06:17:34.458546Z",
     "iopub.status.idle": "2024-04-06T06:17:44.606636Z",
     "shell.execute_reply": "2024-04-06T06:17:44.605685Z",
     "shell.execute_reply.started": "2024-04-06T06:17:34.458883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1af2e08374938adf87c096b08aab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3161704540252686\n",
      "Epoch [2/10], Loss: 2.24755859375\n",
      "Epoch [3/10], Loss: 2.186673164367676\n",
      "Epoch [4/10], Loss: 2.1165056228637695\n",
      "Epoch [5/10], Loss: 2.0432612895965576\n",
      "Epoch [6/10], Loss: 1.9699397087097168\n",
      "Epoch [7/10], Loss: 1.895399570465088\n",
      "Epoch [8/10], Loss: 1.8205984830856323\n",
      "Epoch [9/10], Loss: 1.7479304075241089\n",
      "Epoch [10/10], Loss: 1.6778099536895752\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pre-trained GoogLeNet without the final classification layer\n",
    "googlenet = models.googlenet(pretrained=True).to(device)\n",
    "googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Extract features using GoogLeNet\n",
    "features_list = []\n",
    "labels_list = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "        features_list.append(features)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "# Concatenate features and labels\n",
    "features = torch.cat(features_list, dim=0).to(device)\n",
    "labels = torch.cat(labels_list, dim=0).to(device)\n",
    "\n",
    "# Define the input size for the classifier based on the extracted features\n",
    "input_size = features.size(1)\n",
    "\n",
    "# Initialize the simple classifier and move it to the device\n",
    "classifier = SimpleClassifier(input_size, hidden_size=128, num_classes=10).to(device)\n",
    "# classifier = SimpleClassifier(input_size, hidden_size=128, num_classes=10)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Train the classifier on the extracted features\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = classifier(features)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Save or use the trained classifier for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- fine tuning the feature extracted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:17:56.301385Z",
     "iopub.status.busy": "2024-04-06T12:17:56.300732Z",
     "iopub.status.idle": "2024-04-06T12:17:56.307838Z",
     "shell.execute_reply": "2024-04-06T12:17:56.306859Z",
     "shell.execute_reply.started": "2024-04-06T12:17:56.301356Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_images(path, batch_size):\n",
    "    data_transform = transforms.Compose([transforms.Resize((299,299)), transforms.ToTensor()])\n",
    "    dataset = ImageFolder(path, transform=data_transform)\n",
    "        \n",
    "    data = DataLoader(dataset, batch_size=batch_size) \n",
    "    \n",
    "    X = [] \n",
    "    y = []\n",
    "    \n",
    "    for image, label in tqdm(data):\n",
    "        X.append(image) \n",
    "        y.append(label) \n",
    "        \n",
    "    # Concatenate the lists of arrays along the batch dimension (axis=0)\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3600dfec624f4dab3e66c45e0f7c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad104f493dc84b8f97f2261ea7aa3993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/train\"\n",
    "test_path = \"/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/inaturalist_12K/val\"\n",
    "\n",
    "X_train, y_train = read_images(train_path, 32)\n",
    "X_test, y_test = read_images(test_path, 32)\n",
    "\n",
    "X_train, y_train = shuffle_data(X_train, y_train)\n",
    "\n",
    "train_loader = create_dataloader(X_train, y_train, 32)\n",
    "test_loader = create_dataloader(X_test, y_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data():\n",
    "    \"\"\"\n",
    "    Augment data in a DataLoader using various transformations and return an augmented DataLoader.\n",
    "\n",
    "    Args:\n",
    "    - train_loader (DataLoader): DataLoader containing the original training data.\n",
    "\n",
    "    Returns:\n",
    "    - aug_loader (DataLoader): Augmented DataLoader with transformed data for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the original train_loader\n",
    "    train_loader_copy = copy.deepcopy(train_loader)\n",
    "    \n",
    "    # Define data augmentation transformations\n",
    "    augmented_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "        transforms.RandomRotation(10),  # Randomly rotate the image by up to 10 degrees\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly adjust brightness, contrast, saturation, and hue\n",
    "        transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    ])\n",
    "    \n",
    "    # Apply the transformations to the images in train_loader\n",
    "    train_loader_copy.dataset.transform = augmented_transform\n",
    "\n",
    "    augmented_dataset = ConcatDataset([train_loader.dataset, train_loader_copy.dataset])\n",
    "    aug_loader = DataLoader(augmented_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "    return aug_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:22:33.563289Z",
     "iopub.status.busy": "2024-04-06T12:22:33.562926Z",
     "iopub.status.idle": "2024-04-06T12:22:33.571837Z",
     "shell.execute_reply": "2024-04-06T12:22:33.570843Z",
     "shell.execute_reply.started": "2024-04-06T12:22:33.563259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a smaller feedforward neural network for classification\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, activation_func, apply_dropout, prob, hidden_size, num_classes):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # trying different activation func\n",
    "        if activation_func == \"ReLU\": self.activation = nn.ReLU()\n",
    "        elif activation_func == \"SiLU\": self.activation = nn.SiLU()\n",
    "        elif activation_func == \"GELU\": self.activation = nn.GELU()\n",
    "        elif activation_func == \"Mish\": self.activation = nn.Mish()\n",
    "               \n",
    "        self.apply_drop = apply_dropout\n",
    "        # Adding Dropout\n",
    "        self.dropout = nn.Dropout(p=prob)\n",
    "        \n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.apply_drop == \"Yes\":\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:38:55.131403Z",
     "iopub.status.busy": "2024-04-06T12:38:55.131074Z",
     "iopub.status.idle": "2024-04-06T12:38:55.150496Z",
     "shell.execute_reply": "2024-04-06T12:38:55.149418Z",
     "shell.execute_reply.started": "2024-04-06T12:38:55.131378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "def extract_features():\n",
    "    # Load pre-trained GoogLeNet without the final classification layer\n",
    "    googlenet = models.googlenet(pretrained=True).to(device)\n",
    "    googlenet = nn.Sequential(*list(googlenet.children())[:-1])  # Remove the final layer\n",
    "\n",
    "    # Extract features using GoogLeNet\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Remove the batch dimension\n",
    "            features_list.append(features)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "    # Concatenate features and labels\n",
    "    features = torch.cat(features_list, dim=0).to(device)\n",
    "    labels = torch.cat(labels_list, dim=0).to(device)\n",
    "    \n",
    "    return googlenet, features, labels\n",
    "\n",
    "\n",
    "def evaluate_model(googlenet, classifier, test_loader):\n",
    "    # Initialize lists to store predicted labels and ground truth labels\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Switch the model to evaluation mode\n",
    "    classifier.eval()\n",
    "\n",
    "    # Iterate over the test_loader\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            features = googlenet(images).squeeze()  # Extract features using GoogLeNet\n",
    "            outputs = classifier(features)  # Get predictions from the classifier\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predicted labels\n",
    "            predicted_labels.extend(predicted.cpu().numpy())  # Append predicted labels to the list\n",
    "            true_labels.extend(labels.cpu().numpy())  # Append true labels to the list\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier analysis\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "    # print(f'Testing Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "    \n",
    "    \n",
    "def train_model(config):\n",
    "    if config.data_augment == \"Yes\":\n",
    "        data_loader = augment_data()\n",
    "        train_loader = data_loader\n",
    "    \n",
    "    googlenet, features, labels = extract_features()\n",
    "\n",
    "    # Define the input size for the classifier based on the extracted features\n",
    "    input_size = features.size(1)\n",
    "    \n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "    # Initialize the simple classifier and move it to the device\n",
    "    classifier = SimpleClassifier(input_size, config.activation_func, config.dropout, config.prob, config.hidden_units, num_classes=10).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Trying Different Optimizers \n",
    "    if config.optimizer == \"SGD\": optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"Adam\": optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"NAdam\": optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001) \n",
    "    elif config.optimizer == \"RMSprop\": optimizer = torch.optim.RMSprop(classifier.parameters(), lr=0.001) \n",
    "        \n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.NAdam(classifier.parameters(), lr=0.001)\n",
    "    # optimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n",
    "    # Best Optimizer working is Adam for this problem so trying to change parameters values\n",
    "    # optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=0.0005)\n",
    "    \n",
    "    run_name = f\"epoch_{config.epoch}_opt_{config.optimizer}_act_{config.activation_func}_augment_{config.data_augment}_dropout_{config.dropout}_prob_{config.prob}_hu_{config.hidden_units}\"\n",
    "\n",
    "\n",
    "    # Train the classifier on the extracted features\n",
    "    num_epochs = config.epoch # for 100 epoch this gives accuracy trian_accuracy of 89.52 %\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()  # Set the model to training mode\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        train_accuracy = correct / labels.size(0) * 100\n",
    "        test_accuracy = evaluate_model(googlenet, classifier, test_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy}')\n",
    "        wandb.log({\"train_accuracy\":train_accuracy, 'train_loss':loss.item(), 'test_accuracy':test_accuracy})\n",
    "        \n",
    "    wandb.run.name = run_name\n",
    "    wandb.run.save()\n",
    "    wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:39:01.543966Z",
     "iopub.status.busy": "2024-04-06T12:39:01.543269Z",
     "iopub.status.idle": "2024-04-06T12:39:01.550515Z",
     "shell.execute_reply": "2024-04-06T12:39:01.549351Z",
     "shell.execute_reply.started": "2024-04-06T12:39:01.543930Z"
    }
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "\"name\": \"PartB_FineTuning\",\n",
    "\"metric\": {\n",
    "    \"name\":\"test_accuracy\",\n",
    "    \"goal\": \"maximize\"\n",
    "},\n",
    "\"method\": \"bayes\",\n",
    "\"parameters\": {\n",
    "        \"epoch\": {\n",
    "            \"values\": [10, 20, 30]\n",
    "        },\n",
    "        \"activation_func\": {\n",
    "            \"values\": [\"ReLU\", \"GELU\", \"SiLU\", \"Mish\"]\n",
    "        },\n",
    "        \"data_augment\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"dropout\": {\n",
    "            \"values\": [\"Yes\", \"No\"]\n",
    "        },\n",
    "        \"prob\": {\n",
    "            \"values\": [0.2, 0.3]\n",
    "        },\n",
    "        \"hidden_units\": {\n",
    "            \"values\": [256, 512, 1024]\n",
    "        },\n",
    "        \"optimizer\": {\n",
    "            \"values\": [\"SGD\", \"Adam\", \"NAdam\", \"RMSprop\"]\n",
    "        }\n",
    "    \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T12:39:02.259261Z",
     "iopub.status.busy": "2024-04-06T12:39:02.258694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4twc3txa\n",
      "Sweep URL: https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/4twc3txa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s9b55m8e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_func: ReLU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augment: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tprob: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkadlakpratik\u001b[0m (\u001b[33mspace_monkeys\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/pratikkadlak/Pratik/DeepLearning/DL_Assignment_2/Convolution-Neural-Network/PartB/wandb/run-20240406_190224-s9b55m8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/s9b55m8e' target=\"_blank\">zesty-sweep-1</a></strong> to <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/4twc3txa' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/4twc3txa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/4twc3txa' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/sweeps/4twc3txa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/s9b55m8e' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/s9b55m8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pratikkadlak/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/pratikkadlak/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1488c5f6e4de49e7808e0241b2658df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b35e52d95a0437c83b58d37bbeefd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.3124747276306152, Accuracy: 10.92%, Test Accuracy: 28.449999999999996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ddba79c36451d83524d8216a94fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 2.175415515899658, Accuracy: 28.82%, Test Accuracy: 52.849999999999994\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefd23288e2c4c739047155b4dc12492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 1.9889963865280151, Accuracy: 52.54%, Test Accuracy: 46.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee583ef17f64b6195a3288ba8258661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 1.8581780195236206, Accuracy: 46.56%, Test Accuracy: 61.45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f05568264d4d3190487cccdf8731f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.6742401123046875, Accuracy: 63.87%, Test Accuracy: 62.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789642d597164feabf450cb2c7c2a35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 1.5388100147247314, Accuracy: 62.61%, Test Accuracy: 63.74999999999999\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c3ec933c447cc82df73ffeaf67a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 1.4139257669448853, Accuracy: 64.41%, Test Accuracy: 65.55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f410b3ecab1460f9615d5a536860aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 1.2965381145477295, Accuracy: 66.75%, Test Accuracy: 65.35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4937495644476f9fea44f7dcdf4e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 1.2285629510879517, Accuracy: 65.78%, Test Accuracy: 66.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c45b4a7484e72b511fd800d8a3df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 1.141816258430481, Accuracy: 67.53%, Test Accuracy: 67.25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td></td></tr><tr><td>train_accuracy</td><td></td></tr><tr><td>train_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>67.25</td></tr><tr><td>train_accuracy</td><td>67.52675</td></tr><tr><td>train_loss</td><td>1.14182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-sweep-1</strong> at: <a href='https://wandb.ai/space_monkeys/DL_Assignment_2/runs/s9b55m8e' target=\"_blank\">https://wandb.ai/space_monkeys/DL_Assignment_2/runs/s9b55m8e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240406_190224-s9b55m8e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Initialize a Weights & Biases run and train a CNN model using the configured hyperparameters.\n",
    "\n",
    "    Uses the `wandb.sweep` function to create a sweep with the specified configuration,\n",
    "    then runs the training process using the `wandb.agent` function.\n",
    "    \"\"\"\n",
    "    with wandb.init(project=\"DL_Assignment_2\") as run:\n",
    "        config = wandb.config\n",
    "        train_model(config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project = \"DL_Assignment_2\")\n",
    "wandb.agent(sweep_id, train, count = 10)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T11:58:22.199187Z",
     "iopub.status.busy": "2024-04-06T11:58:22.198831Z",
     "iopub.status.idle": "2024-04-06T11:58:23.249815Z",
     "shell.execute_reply": "2024-04-06T11:58:23.248948Z",
     "shell.execute_reply.started": "2024-04-06T11:58:22.199160Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4722659,
     "sourceId": 8015804,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
